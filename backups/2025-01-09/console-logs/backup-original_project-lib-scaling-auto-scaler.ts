/**
 * ìë™ ìŠ¤ì¼€ì¼ë§ ì„œë¹„ìŠ¤ - ë™ì‹œì ‘ì† 1ë§Œëª… ì§€ì›ì„ ìœ„í•œ ìë™ í™•ì¥/ì¶•ì†Œ
 */

import Redis from 'ioredis'
import { systemMonitor } from '../monitoring/system-monitor'
import { performanceTracker } from '../monitoring/performance-tracker'

const redis = new Redis(process.env.REDIS_URL!)

export interface ScalingConfig {
  minInstances: number
  maxInstances: number
  targetCpuUtilization: number
  targetMemoryUtilization: number
  targetResponseTime: number
  cooldownPeriod: number // ì´ˆ
  scaleUpMultiplier: number
  scaleDownMultiplier: number
}

export interface ScalingDecision {
  action: 'scale_up' | 'scale_down' | 'no_action'
  currentInstances: number
  targetInstances: number
  reason: string
  confidence: number
  timestamp: number
  metrics: {
    cpu: number
    memory: number
    responseTime: number
    concurrentUsers: number
    errorRate: number
  }
}

export interface LoadBalancerConfig {
  algorithm: 'round_robin' | 'least_connections' | 'weighted' | 'ip_hash'
  healthCheckInterval: number
  healthCheckTimeout: number
  maxRetries: number
}

export class AutoScalerService {
  private readonly config: ScalingConfig = {
    minInstances: 2,
    maxInstances: 50,
    targetCpuUtilization: 70,
    targetMemoryUtilization: 80,
    targetResponseTime: 1000,
    cooldownPeriod: 300, // 5ë¶„
    scaleUpMultiplier: 1.5,
    scaleDownMultiplier: 0.8
  }

  private readonly loadBalancerConfig: LoadBalancerConfig = {
    algorithm: 'least_connections',
    healthCheckInterval: 30000, // 30ì´ˆ
    healthCheckTimeout: 5000, // 5ì´ˆ
    maxRetries: 3
  }

  /**
   * ìŠ¤ì¼€ì¼ë§ ê²°ì • ë¡œì§
   */
  async evaluateScaling(): Promise<ScalingDecision> {
    // í˜„ì¬ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    const metrics = await systemMonitor.collectMetrics()
    const performanceData = await performanceTracker.getDashboardData()
    const currentInstances = await this.getCurrentInstanceCount()

    // ì¿¨ë‹¤ìš´ ì²´í¬
    const lastScalingTime = await redis.get('last_scaling_time')
    if (lastScalingTime && Date.now() - parseInt(lastScalingTime) < this.config.cooldownPeriod * 1000) {
      return this.createDecision('no_action', currentInstances, currentInstances, 
        'Cooldown period active', 0, metrics, performanceData.concurrent_users || 0)
    }

    // ìŠ¤ì¼€ì¼ë§ ì ìˆ˜ ê³„ì‚°
    const scalingScore = this.calculateScalingScore(metrics, performanceData, currentInstances)
    
    let action: 'scale_up' | 'scale_down' | 'no_action' = 'no_action'
    let targetInstances = currentInstances
    let reason = 'Metrics within normal range'
    let confidence = scalingScore.confidence

    // ìŠ¤ì¼€ì¼ ì—… ì¡°ê±´
    if (scalingScore.upScore > 0.7 && currentInstances < this.config.maxInstances) {
      action = 'scale_up'
      targetInstances = Math.min(
        Math.ceil(currentInstances * this.config.scaleUpMultiplier),
        this.config.maxInstances
      )
      reason = scalingScore.upReason
    }
    // ìŠ¤ì¼€ì¼ ë‹¤ìš´ ì¡°ê±´
    else if (scalingScore.downScore > 0.7 && currentInstances > this.config.minInstances) {
      action = 'scale_down'
      targetInstances = Math.max(
        Math.floor(currentInstances * this.config.scaleDownMultiplier),
        this.config.minInstances
      )
      reason = scalingScore.downReason
    }

    const decision = this.createDecision(action, currentInstances, targetInstances, 
      reason, confidence, metrics, performanceData.concurrent_users || 0)

    // ê²°ì • ë¡œê·¸ ì €ì¥
    await this.logScalingDecision(decision)

    // ì‹¤ì œ ìŠ¤ì¼€ì¼ë§ ì‹¤í–‰
    if (action !== 'no_action') {
      await this.executeScaling(decision)
    }

    return decision
  }

  /**
   * ìŠ¤ì¼€ì¼ë§ ì ìˆ˜ ê³„ì‚°
   */
  private calculateScalingScore(metrics: any, performanceData: any, currentInstances: number) {
    const scores = {
      cpu: metrics.server.cpuUsage / 100,
      memory: metrics.server.memoryUsage / 100,
      responseTime: Math.min(metrics.application.responseTime / this.config.targetResponseTime, 2),
      concurrentUsers: Math.min((performanceData.concurrent_users || 0) / 8000, 1.5), // 8000ëª… ê¸°ì¤€
      errorRate: Math.min(metrics.application.errorRate / 5, 1), // 5% ê¸°ì¤€
      instanceUtilization: currentInstances / this.config.maxInstances
    }

    // ìŠ¤ì¼€ì¼ ì—… ì ìˆ˜ ê³„ì‚° (ë†’ì„ìˆ˜ë¡ ìŠ¤ì¼€ì¼ ì—… í•„ìš”)
    const upFactors = [
      { weight: 0.3, score: scores.cpu > 0.8 ? scores.cpu : 0 },
      { weight: 0.25, score: scores.memory > 0.85 ? scores.memory : 0 },
      { weight: 0.2, score: scores.responseTime > 1.5 ? scores.responseTime - 1 : 0 },
      { weight: 0.15, score: scores.concurrentUsers > 0.8 ? scores.concurrentUsers : 0 },
      { weight: 0.1, score: scores.errorRate > 0.02 ? scores.errorRate * 2 : 0 }
    ]

    const upScore = upFactors.reduce((sum, factor) => sum + (factor.score * factor.weight), 0)
    
    // ìŠ¤ì¼€ì¼ ë‹¤ìš´ ì ìˆ˜ ê³„ì‚° (ë†’ì„ìˆ˜ë¡ ìŠ¤ì¼€ì¼ ë‹¤ìš´ ê°€ëŠ¥)
    const downFactors = [
      { weight: 0.3, score: scores.cpu < 0.3 ? 1 - scores.cpu : 0 },
      { weight: 0.25, score: scores.memory < 0.4 ? 1 - scores.memory : 0 },
      { weight: 0.2, score: scores.responseTime < 0.5 ? 1 - scores.responseTime : 0 },
      { weight: 0.15, score: scores.concurrentUsers < 0.3 ? 1 - scores.concurrentUsers : 0 },
      { weight: 0.1, score: scores.errorRate < 0.01 ? 1 - scores.errorRate : 0 }
    ]

    const downScore = downFactors.reduce((sum, factor) => sum + (factor.score * factor.weight), 0)

    // ì‹ ë¢°ë„ ê³„ì‚°
    const confidence = Math.min(Math.max(upScore, downScore) + 0.1, 1.0)

    return {
      upScore: Math.min(upScore, 1.0),
      downScore: Math.min(downScore, 1.0),
      confidence,
      upReason: this.getScaleUpReason(scores),
      downReason: this.getScaleDownReason(scores),
      scores
    }
  }

  private getScaleUpReason(scores: any): string {
    const reasons: string[] = []
    if (scores.cpu > 0.8) reasons.push(`High CPU usage (${Math.round(scores.cpu * 100)}%)`)
    if (scores.memory > 0.85) reasons.push(`High memory usage (${Math.round(scores.memory * 100)}%)`)
    if (scores.responseTime > 1.5) reasons.push(`Slow response time (${Math.round(scores.responseTime * 1000)}ms)`)
    if (scores.concurrentUsers > 0.8) reasons.push(`High concurrent users (${Math.round(scores.concurrentUsers * 8000)})`)
    if (scores.errorRate > 0.02) reasons.push(`High error rate (${Math.round(scores.errorRate * 100)}%)`)
    
    return reasons.length > 0 ? reasons.join(', ') : 'Multiple metrics above thresholds'
  }

  private getScaleDownReason(scores: any): string {
    const reasons: string[] = []
    if (scores.cpu < 0.3) reasons.push(`Low CPU usage (${Math.round(scores.cpu * 100)}%)`)
    if (scores.memory < 0.4) reasons.push(`Low memory usage (${Math.round(scores.memory * 100)}%)`)
    if (scores.responseTime < 0.5) reasons.push(`Fast response time (${Math.round(scores.responseTime * 1000)}ms)`)
    if (scores.concurrentUsers < 0.3) reasons.push(`Low concurrent users (${Math.round(scores.concurrentUsers * 8000)})`)
    
    return reasons.length > 0 ? reasons.join(', ') : 'All metrics below thresholds'
  }

  private createDecision(action: 'scale_up' | 'scale_down' | 'no_action', 
                        current: number, target: number, reason: string, 
                        confidence: number, metrics: any, concurrentUsers: number): ScalingDecision {
    return {
      action,
      currentInstances: current,
      targetInstances: target,
      reason,
      confidence: Math.round(confidence * 100) / 100,
      timestamp: Date.now(),
      metrics: {
        cpu: metrics.server.cpuUsage,
        memory: metrics.server.memoryUsage,
        responseTime: metrics.application.responseTime,
        concurrentUsers: concurrentUsers || 0,
        errorRate: metrics.application.errorRate
      }
    }
  }

  /**
   * ì‹¤ì œ ìŠ¤ì¼€ì¼ë§ ì‹¤í–‰
   */
  private async executeScaling(decision: ScalingDecision) {
    console.log(`ğŸ”„ Scaling ${decision.action}: ${decision.currentInstances} â†’ ${decision.targetInstances}`)
    console.log(`ğŸ“Š Reason: ${decision.reason}`)
    console.log(`ğŸ¯ Confidence: ${decision.confidence}`)

    try {
      if (decision.action === 'scale_up') {
        await this.scaleUp(decision.targetInstances - decision.currentInstances)
      } else if (decision.action === 'scale_down') {
        await this.scaleDown(decision.currentInstances - decision.targetInstances)
      }

      // ìŠ¤ì¼€ì¼ë§ ì‹œê°„ ê¸°ë¡
      await redis.set('last_scaling_time', Date.now().toString())

      // ì„±ê³µ ë¡œê·¸
      console.log(`âœ… Scaling completed successfully`)
      
    } catch (error) {
      console.error(`âŒ Scaling failed:`, error)
      // ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥
      await redis.lpush('scaling_errors', JSON.stringify({
        decision,
        error: error instanceof Error ? error.message : String(error),
        timestamp: Date.now()
      }))
    }
  }

  /**
   * ìŠ¤ì¼€ì¼ ì—… ì‹¤í–‰
   */
  private async scaleUp(instancesToAdd: number) {
    // ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Kubernetes, Docker, AWS Auto Scaling ë“±ì„ ì‚¬ìš©
    console.log(`ğŸš€ Starting ${instancesToAdd} new instances...`)
    
    for (let i = 0; i < instancesToAdd; i++) {
      const instanceId = `instance_${Date.now()}_${i}`
      
      // ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘ ì‹œë®¬ë ˆì´ì…˜
      await this.startInstance(instanceId)
      
      // í—¬ìŠ¤ ì²´í¬ ëŒ€ê¸°
      await this.waitForHealthy(instanceId)
      
      // ë¡œë“œ ë°¸ëŸ°ì„œì— ì¶”ê°€
      await this.addToLoadBalancer(instanceId)
      
      console.log(`ğŸ“¦ Instance ${instanceId} is now ready and serving traffic`)
    }

    // ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ ì—…ë°ì´íŠ¸
    await this.updateInstanceCount(instancesToAdd)
  }

  /**
   * ìŠ¤ì¼€ì¼ ë‹¤ìš´ ì‹¤í–‰
   */
  private async scaleDown(instancesToRemove: number) {
    console.log(`ğŸ›‘ Stopping ${instancesToRemove} instances...`)
    
    // ê°€ì¥ ì ê²Œ ì‚¬ìš©ë˜ëŠ” ì¸ìŠ¤í„´ìŠ¤ë¶€í„° ì œê±°
    const instancesToStop = await this.selectInstancesForRemoval(instancesToRemove)
    
    for (const instanceId of instancesToStop) {
      // ë¡œë“œ ë°¸ëŸ°ì„œì—ì„œ ì œê±° (ë“œë ˆì¸)
      await this.removeFromLoadBalancer(instanceId)
      
      // ê¸°ì¡´ ì—°ê²° ë“œë ˆì¸ ëŒ€ê¸°
      await this.drainConnections(instanceId)
      
      // ì¸ìŠ¤í„´ìŠ¤ ì¤‘ì§€
      await this.stopInstance(instanceId)
      
      console.log(`ğŸ—‘ï¸ Instance ${instanceId} has been stopped`)
    }

    // ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ ì—…ë°ì´íŠ¸
    await this.updateInstanceCount(-instancesToRemove)
  }

  /**
   * ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘ (ì‹œë®¬ë ˆì´ì…˜)
   */
  private async startInstance(instanceId: string) {
    // ì‹¤ì œë¡œëŠ” Docker ì»¨í…Œì´ë„ˆ ì‹œì‘ ë˜ëŠ” VM ìƒì„±
    await redis.hset('instances', instanceId, JSON.stringify({
      status: 'starting',
      createdAt: Date.now(),
      health: 'unknown'
    }))

    // ì‹œì‘ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
    await new Promise(resolve => setTimeout(resolve, 5000))

    await redis.hset('instances', instanceId, JSON.stringify({
      status: 'running',
      createdAt: Date.now(),
      health: 'healthy'
    }))
  }

  /**
   * ì¸ìŠ¤í„´ìŠ¤ í—¬ìŠ¤ ì²´í¬ ëŒ€ê¸°
   */
  private async waitForHealthy(instanceId: string) {
    let attempts = 0
    const maxAttempts = 20

    while (attempts < maxAttempts) {
      const instanceData = await redis.hget('instances', instanceId)
      if (instanceData) {
        const instance = JSON.parse(instanceData)
        if (instance.health === 'healthy') {
          return true
        }
      }

      await new Promise(resolve => setTimeout(resolve, 3000))
      attempts++
    }

    throw new Error(`Instance ${instanceId} failed health check`)
  }

  /**
   * ë¡œë“œ ë°¸ëŸ°ì„œì— ì¸ìŠ¤í„´ìŠ¤ ì¶”ê°€
   */
  private async addToLoadBalancer(instanceId: string) {
    await redis.sadd('lb_active_instances', instanceId)
    await redis.hset('lb_instance_weights', instanceId, '1.0')
    
    console.log(`ğŸ”€ Added ${instanceId} to load balancer`)
  }

  /**
   * ë¡œë“œ ë°¸ëŸ°ì„œì—ì„œ ì¸ìŠ¤í„´ìŠ¤ ì œê±°
   */
  private async removeFromLoadBalancer(instanceId: string) {
    await redis.srem('lb_active_instances', instanceId)
    await redis.hdel('lb_instance_weights', instanceId)
    
    console.log(`ğŸš« Removed ${instanceId} from load balancer`)
  }

  /**
   * ê¸°ì¡´ ì—°ê²° ë“œë ˆì¸
   */
  private async drainConnections(instanceId: string) {
    console.log(`â³ Draining connections for ${instanceId}...`)
    
    // ìƒˆ ì—°ê²° ì°¨ë‹¨í•˜ê³  ê¸°ì¡´ ì—°ê²° ì™„ë£Œ ëŒ€ê¸°
    let attempts = 0
    const maxAttempts = 30 // 30ì´ˆ ëŒ€ê¸°

    while (attempts < maxAttempts) {
      const activeConnections = await redis.get(`connections:${instanceId}`) || '0'
      if (parseInt(activeConnections) === 0) {
        console.log(`âœ… All connections drained for ${instanceId}`)
        return
      }

      await new Promise(resolve => setTimeout(resolve, 1000))
      attempts++
    }

    console.log(`âš ï¸ Force draining ${instanceId} after timeout`)
  }

  /**
   * ì¸ìŠ¤í„´ìŠ¤ ì¤‘ì§€
   */
  private async stopInstance(instanceId: string) {
    await redis.hdel('instances', instanceId)
    await redis.del(`connections:${instanceId}`)
    console.log(`ğŸ›‘ Stopped instance ${instanceId}`)
  }

  /**
   * ì œê±°í•  ì¸ìŠ¤í„´ìŠ¤ ì„ íƒ
   */
  private async selectInstancesForRemoval(count: number): Promise<string[]> {
    const allInstances = await redis.smembers('lb_active_instances')
    
    // ì—°ê²° ìˆ˜ê°€ ì ì€ ìˆœìœ¼ë¡œ ì •ë ¬
    const instanceConnections: any[] = []
    for (const instanceId of allInstances) {
      const connections = parseInt(await redis.get(`connections:${instanceId}`) || '0')
      instanceConnections.push({ instanceId, connections })
    }

    instanceConnections.sort((a, b) => a.connections - b.connections)
    
    return instanceConnections.slice(0, count).map(item => item.instanceId)
  }

  /**
   * í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ ì¡°íšŒ
   */
  private async getCurrentInstanceCount(): Promise<number> {
    return await redis.scard('lb_active_instances')
  }

  /**
   * ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ ì—…ë°ì´íŠ¸
   */
  private async updateInstanceCount(delta: number) {
    const currentCount = await this.getCurrentInstanceCount()
    const newCount = Math.max(this.config.minInstances, currentCount + delta)
    
    await redis.set('current_instance_count', newCount)
    console.log(`ğŸ“Š Instance count updated: ${currentCount} â†’ ${newCount}`)
  }

  /**
   * ìŠ¤ì¼€ì¼ë§ ê²°ì • ë¡œê·¸
   */
  private async logScalingDecision(decision: ScalingDecision) {
    await redis.lpush('scaling_decisions', JSON.stringify(decision))
    await redis.ltrim('scaling_decisions', 0, 999) // ìµœê·¼ 1000ê°œë§Œ ìœ ì§€
  }

  /**
   * ìŠ¤ì¼€ì¼ë§ íˆìŠ¤í† ë¦¬ ì¡°íšŒ
   */
  async getScalingHistory(limit = 50) {
    const decisions = await redis.lrange('scaling_decisions', 0, limit - 1)
    return decisions.map(decision => JSON.parse(decision))
  }

  /**
   * ì˜ˆì¸¡ì  ìŠ¤ì¼€ì¼ë§ (íŠ¸ë˜í”½ íŒ¨í„´ ê¸°ë°˜)
   */
  async predictiveScaling() {
    const now = new Date()
    const hour = now.getHours()
    const dayOfWeek = now.getDay()
    
    // ê³¼ê±° ë°ì´í„° ê¸°ë°˜ íŒ¨í„´ ë¶„ì„
    const historicalPattern = await this.getTrafficPattern(hour, dayOfWeek)
    
    if (historicalPattern.expectedIncrease > 1.5) {
      console.log(`ğŸ”® Predictive scaling: Expected ${historicalPattern.expectedIncrease}x traffic increase`)
      
      const currentInstances = await this.getCurrentInstanceCount()
      const recommendedInstances = Math.ceil(currentInstances * historicalPattern.expectedIncrease)
      
      if (recommendedInstances > currentInstances) {
        const decision: ScalingDecision = {
          action: 'scale_up',
          currentInstances,
          targetInstances: Math.min(recommendedInstances, this.config.maxInstances),
          reason: `Predictive scaling for expected traffic increase (${historicalPattern.expectedIncrease}x)`,
          confidence: historicalPattern.confidence,
          timestamp: Date.now(),
          metrics: {
            cpu: 0,
            memory: 0,
            responseTime: 0,
            concurrentUsers: 0,
            errorRate: 0
          }
        }

        await this.executeScaling(decision)
        return decision
      }
    }

    return null
  }

  /**
   * íŠ¸ë˜í”½ íŒ¨í„´ ë¶„ì„
   */
  private async getTrafficPattern(hour: number, dayOfWeek: number) {
    // ê³¼ê±° 30ì¼ê°„ì˜ ë™ì¼ ì‹œê°„ëŒ€ íŠ¸ë˜í”½ ë°ì´í„° ë¶„ì„
    const patterns: number[] = []
    
    for (let i = 1; i <= 30; i++) {
      const key = `traffic_pattern:${dayOfWeek}:${hour}:${i}`
      const traffic = await redis.get(key)
      if (traffic) {
        patterns.push(parseFloat(traffic))
      }
    }

    if (patterns.length < 5) {
      return { expectedIncrease: 1.0, confidence: 0.1 }
    }

    const average = patterns.reduce((a, b) => a + b, 0) / patterns.length
    const currentBaseline = parseFloat(await redis.get('current_traffic_baseline') || '1.0')
    const expectedIncrease = average / currentBaseline

    // ì‹ ë¢°ë„ ê³„ì‚° (ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜ì™€ ë³€ë™ì„± ê¸°ë°˜)
    const variance = patterns.reduce((sum, val) => sum + Math.pow(val - average, 2), 0) / patterns.length
    const confidence = Math.min(patterns.length / 30, 1.0) * (1 - Math.min(variance / average, 1.0))

    return {
      expectedIncrease: Math.max(expectedIncrease, 1.0),
      confidence: Math.round(confidence * 100) / 100
    }
  }

  /**
   * ë¡œë“œ ë°¸ëŸ°ì„œ ìƒíƒœ ê´€ë¦¬
   */
  async manageLoadBalancer() {
    const activeInstances = await redis.smembers('lb_active_instances')
    const healthyInstances: string[] = []

    // ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ í—¬ìŠ¤ ì²´í¬
    for (const instanceId of activeInstances) {
      const isHealthy = await this.checkInstanceHealth(instanceId)
      if (isHealthy) {
        healthyInstances.push(instanceId)
      } else {
        console.log(`ğŸš¨ Unhealthy instance detected: ${instanceId}`)
        await this.handleUnhealthyInstance(instanceId)
      }
    }

    // í—¬ì‹œ ì¸ìŠ¤í„´ìŠ¤ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
    await this.updateLoadBalancingWeights(healthyInstances)

    return {
      total: activeInstances.length,
      healthy: healthyInstances.length,
      unhealthy: activeInstances.length - healthyInstances.length
    }
  }

  private async checkInstanceHealth(instanceId: string): Promise<boolean> {
    // ì‹¤ì œ í—¬ìŠ¤ ì²´í¬ êµ¬í˜„
    const responseTime = Math.random() * 100 // ì‹œë®¬ë ˆì´ì…˜
    const isResponding = Math.random() > 0.05 // 95% ì •ìƒ

    return responseTime < 50 && isResponding
  }

  private async handleUnhealthyInstance(instanceId: string) {
    // ë¶ˆê±´ì „í•œ ì¸ìŠ¤í„´ìŠ¤ ì²˜ë¦¬
    await this.removeFromLoadBalancer(instanceId)
    
    // ì¬ì‹œì‘ ì‹œë„
    try {
      await this.restartInstance(instanceId)
      await this.waitForHealthy(instanceId)
      await this.addToLoadBalancer(instanceId)
      console.log(`ğŸ”„ Successfully restarted ${instanceId}`)
    } catch (error) {
      console.error(`âŒ Failed to restart ${instanceId}:`, error)
      await this.stopInstance(instanceId)
    }
  }

  private async restartInstance(instanceId: string) {
    await this.stopInstance(instanceId)
    await new Promise(resolve => setTimeout(resolve, 2000))
    await this.startInstance(instanceId)
  }

  private async updateLoadBalancingWeights(healthyInstances: string[]) {
    // ì¸ìŠ¤í„´ìŠ¤ë³„ í˜„ì¬ ë¶€í•˜ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ì¡°ì •
    for (const instanceId of healthyInstances) {
      const currentLoad = await this.getInstanceLoad(instanceId)
      const weight = Math.max(0.1, 1.0 - (currentLoad * 0.8)) // ë¶€í•˜ê°€ ë†’ì„ìˆ˜ë¡ ê°€ì¤‘ì¹˜ ê°ì†Œ
      
      await redis.hset('lb_instance_weights', instanceId, weight.toString())
    }
  }

  private async getInstanceLoad(instanceId: string): Promise<number> {
    const connections = parseInt(await redis.get(`connections:${instanceId}`) || '0')
    const maxConnections = 1000 // ì¸ìŠ¤í„´ìŠ¤ë‹¹ ìµœëŒ€ ì—°ê²° ìˆ˜
    
    return Math.min(connections / maxConnections, 1.0)
  }
}

export const autoScaler = new AutoScalerService()